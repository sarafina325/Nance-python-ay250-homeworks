{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction with the World Homework (#3)\n",
    "Python Computing for Data Science (c) J Bloom, UC Berkeley, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monty: The Python Siri\n",
    "\n",
    "Let's make a Siri-like program with the following properties:\n",
    "   - record your voice command\n",
    "   - use a webservice to parse that sound file into text\n",
    "   - based on what the text, take three different types of actions:\n",
    "       - send an email to yourself\n",
    "       - do some math\n",
    "       - tell a joke\n",
    "\n",
    "So for example, if you say \"Monty: email me with subject hello and body goodbye\", it will email you with the appropriate subject and body. If you say \"Monty: tell me a joke\" then it will go to the web and find a joke and print it for you. If you say, \"Monty: calculate two times three\" it should response with printing the number 6.\n",
    "\n",
    "Hint: you can use speed-to-text apps like Houndify to return the text (but not do the actions). You'll need to sign up for a free API and then follow documentation instructions for using the service within Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* done recording\n",
      "new2.wav: wrong sample width (must be 16-bit)\n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: \n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: \n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Partial transcript: shh\n",
      "Final response: {'Format': 'SoundHoundVoiceSearchResult', 'FormatVersion': '1.0', 'Status': 'OK', 'NumToReturn': 1, 'AllResults': [{'CommandKind': 'NoResultCommand', 'SpokenResponse': '', 'SpokenResponseLong': \"Didn't get that!\", 'WrittenResponse': \"Didn't get that!\", 'WrittenResponseLong': \"Didn't get that!\", 'AutoListen': False, 'ViewType': ['Native', 'None'], 'TranscriptionSearchURL': 'http://www.google.com/#q=shh'}], 'Disambiguation': {'NumToShow': 1, 'ChoiceData': [{'Transcription': 'shh', 'ConfidenceScore': 1, 'FixedTranscription': ''}]}, 'ResultsAreFinal': [True], 'DomainUsage': [], 'BuildInfo': {'User': 'knightly', 'Date': 'Thu Jan 18 04:19:10 PST 2018', 'Machine': '7fhpjh2.pnp.melodis.com', 'SVNRevision': '39685', 'SVNBranch': 'dev_multi_machine', 'BuildNumber': '4784', 'Kind': 'Low Fat', 'Variant': 'release'}, 'QueryID': 'ec3029b1-87fb-4845-8991-30881dae1726', 'ServerGeneratedId': 'ec3029b1-87fb-4845-8991-30881dae1726', 'AudioLength': 5.170000076293945, 'RealSpeechTime': 2.862, 'RealTime': 2.865}\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "chunk = 1024\n",
    "FORMAT = pyaudio.paInt32\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "RECORD_SECONDS = 6\n",
    "WAVE_OUTPUT_FILENAME = \"new2.wav\"\n",
    "p = pyaudio.PyAudio()\n",
    "stream = p.open(format = FORMAT,\n",
    "    channels = CHANNELS,\n",
    "    rate = RATE,\n",
    "    input = True,\n",
    "    frames_per_buffer = chunk)\n",
    "all = []\n",
    "for i in range(0, int(RATE / chunk * RECORD_SECONDS)):\n",
    "    data = stream.read(chunk)\n",
    "    all.append(data)\n",
    "print(\"* done recording\")\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "waveFile.setnchannels(CHANNELS)\n",
    "waveFile.setsampwidth(p.get_sample_size(FORMAT))\n",
    "waveFile.setframerate(RATE)\n",
    "waveFile.writeframes(b''.join(all))\n",
    "waveFile.close()\n",
    "\n",
    "import houndify\n",
    "import os\n",
    "import time\n",
    "\n",
    "os.environ['HOUNDIFY_API_CLIENT']='QfZC7UHC__Se1iiCpJdGbg=='\n",
    "os.environ['HOUNDIFY_API_KEY']='3uCW49-HHTKw4xt2MizSe5XfIa3TrkePApdYMWBtBfWN6bC79ISWPzdwRnZ-NQwu3eyswc_M8SPjodehm20aFQ=='\n",
    "\n",
    "CLIENT_ID = os.environ.get('HOUNDIFY_API_CLIENT')\n",
    "CLIENT_KEY = os.environ.get('HOUNDIFY_API_KEY')\n",
    "BUFFER_SIZE = 512\n",
    "\n",
    "#\n",
    "# Simplest HoundListener; just print out what we receive.\n",
    "# You can use these callbacks to interact with your UI.\n",
    "#\n",
    "class MyListener(houndify.HoundListener):\n",
    "  def onPartialTranscript(self, transcript):\n",
    "    print(\"Partial transcript: \" + transcript)\n",
    "  def onFinalResponse(self, response):\n",
    "    print(\"Final response: \" + str(response))\n",
    "  def onError(self, err):\n",
    "    print(\"Error: \" + str(err))\n",
    "\n",
    "\n",
    "client = houndify.StreamingHoundClient(CLIENT_ID, CLIENT_KEY, \"test_user\")\n",
    "client.setLocation(37.388309, -121.973968)\n",
    "\n",
    "\n",
    "audio = wave.open(WAVE_OUTPUT_FILENAME)\n",
    "if audio.getsampwidth() != 2:\n",
    "  print(\"%s: wrong sample width (must be 16-bit)\" % WAVE_OUTPUT_FILENAME)\n",
    "if audio.getframerate() != 8000 and audio.getframerate() != 16000:\n",
    "  print(\"%s: unsupported sampling frequency (must be either 8 or 16 khz)\" % WAVE_OUTPUT_FILENAME)\n",
    "if audio.getnchannels() != 1:\n",
    "  print(\"%s: must be single channel (mono)\" % WAVE_OUTPUT_FILENAME)\n",
    "\n",
    "client.setSampleRate(audio.getframerate())\n",
    "client.start(MyListener())\n",
    "\n",
    "while True:\n",
    "  samples = audio.readframes(BUFFER_SIZE)\n",
    "  if len(samples) == 0: break\n",
    "  if client.fill(samples): break\n",
    "  time.sleep(0.032) # simulate real-time so we can see the partial transcripts\n",
    "       \n",
    "result = client.finish() # returns either final response or error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Write a program that identifies musical notes from sound (AIFF) files. \n",
    "\n",
    "  - Run it on the supplied sound files (12) and report your program’s results. \n",
    "  - Use the labeled sounds (4) to make sure it works correctly. The provided sound files contain 1-3 simultaneous notes from different organs.\n",
    "  - Save copies of any example plots to illustrate how your program works.\n",
    "  \n",
    "  https://piazza.com/berkeley/fall2016/ay250/resources -> hw3_sound_files.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints: You’ll want to decompose the sound into a frequency power spectrum. Use a Fast Fourier Transform. Be care about “unpacking” the string hexcode into python data structures. The sound files use 32 bit data. Play around with what happens when you convert the string data to other integer sizes, or signed vs unsigned integers. Also, beware of harmonics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
